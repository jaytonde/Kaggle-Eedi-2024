{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a169e9a4",
   "metadata": {
    "_cell_guid": "fd1b354a-917c-4776-9773-ee2d22996d25",
    "_uuid": "6b9078b5-d216-4709-afd2-27615a5a3a57",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-01T03:06:38.351767Z",
     "iopub.status.busy": "2024-12-01T03:06:38.351513Z",
     "iopub.status.idle": "2024-12-01T03:10:10.763082Z",
     "shell.execute_reply": "2024-12-01T03:10:10.762152Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 212.420272,
     "end_time": "2024-12-01T03:10:10.764806",
     "exception": false,
     "start_time": "2024-12-01T03:06:38.344534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.4.0\r\n",
      "Uninstalling torch-2.4.0:\r\n",
      "  Successfully uninstalled torch-2.4.0\r\n",
      "Processing /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl\r\n",
      "Installing collected packages: logits-processor-zoo\r\n",
      "Successfully installed logits-processor-zoo-0.1.0\r\n",
      "CPU times: user 2.13 s, sys: 550 ms, total: 2.68 s\n",
      "Wall time: 3min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip uninstall -y torch\n",
    "!pip install -q --no-index --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-vllm vllm\n",
    "!pip install -q -U /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install -q -U /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n",
    "!pip install -q --no-deps --no-index /kaggle/input/hf-libraries/sentence-transformers/sentence_transformers-3.1.0-py3-none-any.whl\n",
    "!pip install --no-deps --no-index /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c155bf32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T03:10:10.775021Z",
     "iopub.status.busy": "2024-12-01T03:10:10.774728Z",
     "iopub.status.idle": "2024-12-01T03:10:19.201619Z",
     "shell.execute_reply": "2024-12-01T03:10:19.200674Z"
    },
    "papermill": {
     "duration": 8.435017,
     "end_time": "2024-12-01T03:10:19.204451",
     "exception": false,
     "start_time": "2024-12-01T03:10:10.769434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers peft accelerate \\\n",
    "    -q -U --no-index --find-links /kaggle/input/lmsys-wheel-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1640d334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T03:10:19.216529Z",
     "iopub.status.busy": "2024-12-01T03:10:19.215784Z",
     "iopub.status.idle": "2024-12-01T03:11:01.081752Z",
     "shell.execute_reply": "2024-12-01T03:11:01.080798Z"
    },
    "papermill": {
     "duration": 41.874608,
     "end_time": "2024-12-01T03:11:01.083889",
     "exception": false,
     "start_time": "2024-12-01T03:10:19.209281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --no-index /kaggle/input/bitsandbytes0-42-0/bitsandbytes-0.42.0-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n",
    "!pip install --no-index  /kaggle/input/bitsandbytes0-42-0/optimum-1.21.2-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n",
    "!pip install --no-index  /kaggle/input/bitsandbytes0-42-0/auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --find-links=/kaggle/input/bitsandbytes0-42-0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16256672",
   "metadata": {
    "papermill": {
     "duration": 0.00405,
     "end_time": "2024-12-01T03:11:01.092511",
     "exception": false,
     "start_time": "2024-12-01T03:11:01.088461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Retrieval using 7B LLM\n",
    "Taken from: https://www.kaggle.com/code/sayoulala/use-llm-embedding-recall-infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c7fec34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T03:11:01.102430Z",
     "iopub.status.busy": "2024-12-01T03:11:01.102117Z",
     "iopub.status.idle": "2024-12-01T03:11:01.857139Z",
     "shell.execute_reply": "2024-12-01T03:11:01.856214Z"
    },
    "papermill": {
     "duration": 0.762222,
     "end_time": "2024-12-01T03:11:01.858927",
     "exception": false,
     "start_time": "2024-12-01T03:11:01.096705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_text</th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>ConstructName</th>\n",
       "      <th>SubjectName</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>incorrect_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>###question###:BIDMAS-Use the order of operati...</td>\n",
       "      <td>1869_B</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>###question###:BIDMAS-Use the order of operati...</td>\n",
       "      <td>1869_C</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>###question###:BIDMAS-Use the order of operati...</td>\n",
       "      <td>1869_D</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>Does not need brackets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>###question###:Simplifying Algebraic Fractions...</td>\n",
       "      <td>1870_A</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>###question###:Simplifying Algebraic Fractions...</td>\n",
       "      <td>1870_B</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>###question###:Simplifying Algebraic Fractions...</td>\n",
       "      <td>1870_C</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>\\( m-1 \\)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>###question###:Range and Interquartile Range f...</td>\n",
       "      <td>1871_A</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>Only\\nTom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>###question###:Range and Interquartile Range f...</td>\n",
       "      <td>1871_C</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>Both Tom and Katie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>###question###:Range and Interquartile Range f...</td>\n",
       "      <td>1871_D</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>Neither is correct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          query_text QuestionId_Answer  \\\n",
       "0  ###question###:BIDMAS-Use the order of operati...            1869_B   \n",
       "1  ###question###:BIDMAS-Use the order of operati...            1869_C   \n",
       "2  ###question###:BIDMAS-Use the order of operati...            1869_D   \n",
       "3  ###question###:Simplifying Algebraic Fractions...            1870_A   \n",
       "4  ###question###:Simplifying Algebraic Fractions...            1870_B   \n",
       "5  ###question###:Simplifying Algebraic Fractions...            1870_C   \n",
       "6  ###question###:Range and Interquartile Range f...            1871_A   \n",
       "7  ###question###:Range and Interquartile Range f...            1871_C   \n",
       "8  ###question###:Range and Interquartile Range f...            1871_D   \n",
       "\n",
       "                                       ConstructName  \\\n",
       "0  Use the order of operations to carry out calcu...   \n",
       "1  Use the order of operations to carry out calcu...   \n",
       "2  Use the order of operations to carry out calcu...   \n",
       "3  Simplify an algebraic fraction by factorising ...   \n",
       "4  Simplify an algebraic fraction by factorising ...   \n",
       "5  Simplify an algebraic fraction by factorising ...   \n",
       "6            Calculate the range from a list of data   \n",
       "7            Calculate the range from a list of data   \n",
       "8            Calculate the range from a list of data   \n",
       "\n",
       "                                         SubjectName  \\\n",
       "0                                             BIDMAS   \n",
       "1                                             BIDMAS   \n",
       "2                                             BIDMAS   \n",
       "3                    Simplifying Algebraic Fractions   \n",
       "4                    Simplifying Algebraic Fractions   \n",
       "5                    Simplifying Algebraic Fractions   \n",
       "6  Range and Interquartile Range from a List of Data   \n",
       "7  Range and Interquartile Range from a List of Data   \n",
       "8  Range and Interquartile Range from a List of Data   \n",
       "\n",
       "                                        QuestionText         correct_answer  \\\n",
       "0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "1  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "2  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "3  Simplify the following, if possible: \\( \\frac{...      Does not simplify   \n",
       "4  Simplify the following, if possible: \\( \\frac{...      Does not simplify   \n",
       "5  Simplify the following, if possible: \\( \\frac{...      Does not simplify   \n",
       "6  Tom and Katie are discussing the \\( 5 \\) plant...            Only\\nKatie   \n",
       "7  Tom and Katie are discussing the \\( 5 \\) plant...            Only\\nKatie   \n",
       "8  Tom and Katie are discussing the \\( 5 \\) plant...            Only\\nKatie   \n",
       "\n",
       "         incorrect_answer  \n",
       "0  \\( 3 \\times 2+(4-5) \\)  \n",
       "1   \\( 3 \\times(2+4-5) \\)  \n",
       "2  Does not need brackets  \n",
       "3               \\( m+1 \\)  \n",
       "4               \\( m+2 \\)  \n",
       "5               \\( m-1 \\)  \n",
       "6               Only\\nTom  \n",
       "7      Both Tom and Katie  \n",
       "8      Neither is correct  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "\n",
    "\n",
    "rows = []\n",
    "for idx, row in full_df.iterrows():\n",
    "    for option in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        if option == row.CorrectAnswer:\n",
    "            continue\n",
    "            \n",
    "        correct_answer = row[f\"Answer{row.CorrectAnswer}Text\"]\n",
    "\n",
    "        query_text =f\"###question###:{row['SubjectName']}-{row['ConstructName']}-{row['QuestionText']}\\n###Correct Answer###:{correct_answer}\\n###Misconcepte Incorrect answer###:{option}.{row[f'Answer{option}Text']}\"\n",
    "\n",
    "        rows.append({\"query_text\": query_text, \n",
    "                     \"QuestionId_Answer\": f\"{row.QuestionId}_{option}\",\n",
    "                     \"ConstructName\": row.ConstructName,\n",
    "                     \"SubjectName\": row.SubjectName,\n",
    "                     \"QuestionText\": row.QuestionText,\n",
    "                     \"correct_answer\": correct_answer,\n",
    "                     \"incorrect_answer\": row[f\"Answer{option}Text\"]\n",
    "                     })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "767bb70f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T03:11:01.869486Z",
     "iopub.status.busy": "2024-12-01T03:11:01.869192Z",
     "iopub.status.idle": "2024-12-01T03:11:05.734474Z",
     "shell.execute_reply": "2024-12-01T03:11:05.733561Z"
    },
    "papermill": {
     "duration": 3.87282,
     "end_time": "2024-12-01T03:11:05.736459",
     "exception": false,
     "start_time": "2024-12-01T03:11:01.863639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from numpy.linalg import norm\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel,BitsAndBytesConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "def batch_to_device(batch, target_device):\n",
    "    \"\"\"\n",
    "    send a pytorch batch to a device (CPU/GPU)\n",
    "    \"\"\"\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                    attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "def inference(df, model, tokenizer, device):\n",
    "    batch_size = 16\n",
    "    max_length = 512\n",
    "    sentences = list(df['query_text'].values)\n",
    "\n",
    "    all_embeddings = []\n",
    "    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n",
    "    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
    "    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n",
    "        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().tolist()\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n",
    "\n",
    "    return np.concatenate(all_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d713448",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T03:11:05.747154Z",
     "iopub.status.busy": "2024-12-01T03:11:05.746459Z",
     "iopub.status.idle": "2024-12-01T03:11:05.750219Z",
     "shell.execute_reply": "2024-12-01T03:11:05.749422Z"
    },
    "papermill": {
     "duration": 0.010535,
     "end_time": "2024-12-01T03:11:05.751715",
     "exception": false,
     "start_time": "2024-12-01T03:11:05.741180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n",
    "model_path = \"/kaggle/input/sfr-embedding-mistral/SFR-Embedding-2_R\"\n",
    "lora_path=\"/kaggle/input/v7-recall/epoch_19_model/adapter.bin\"\n",
    "device='cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72305e8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T03:11:05.761504Z",
     "iopub.status.busy": "2024-12-01T03:11:05.761212Z",
     "iopub.status.idle": "2024-12-01T03:11:28.386329Z",
     "shell.execute_reply": "2024-12-01T03:11:28.385623Z"
    },
    "papermill": {
     "duration": 22.632065,
     "end_time": "2024-12-01T03:11:28.388255",
     "exception": false,
     "start_time": "2024-12-01T03:11:05.756190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('/kaggle/input/eedi-finetuned-bge-public/Eedi-finetuned-bge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "136b28f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T03:11:28.399102Z",
     "iopub.status.busy": "2024-12-01T03:11:28.398548Z",
     "iopub.status.idle": "2024-12-01T03:11:39.650310Z",
     "shell.execute_reply": "2024-12-01T03:11:39.649517Z"
    },
    "papermill": {
     "duration": 11.258904,
     "end_time": "2024-12-01T03:11:39.651907",
     "exception": false,
     "start_time": "2024-12-01T03:11:28.393003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55a6a25d554499bbcdec0b7c81825fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13d8fc600674fcf997bbf3731b99996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2587, 1024])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.autonotebook import trange\n",
    "\n",
    "\n",
    "task_description               = 'Given a math question and a misconcepte incorrect answer, please retrieve the most accurate reason for the misconception.'\n",
    "\n",
    "\n",
    "V_answer                       = model.encode(df['query_text'], convert_to_tensor=True)\n",
    "\n",
    "misconception_df               = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "misconception_df[\"query_text\"] = misconception_df[\"MisconceptionName\"]\n",
    "\n",
    "V_misconception                = model.encode(misconception_df[\"query_text\"], convert_to_tensor=True)\n",
    "V_misconception.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1974081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T03:11:39.662942Z",
     "iopub.status.busy": "2024-12-01T03:11:39.662665Z",
     "iopub.status.idle": "2024-12-01T03:11:39.818934Z",
     "shell.execute_reply": "2024-12-01T03:11:39.817836Z"
    },
    "papermill": {
     "duration": 0.165618,
     "end_time": "2024-12-01T03:11:39.822666",
     "exception": false,
     "start_time": "2024-12-01T03:11:39.657048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
    "import sys, os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# def get_matches(V_topic, V_content, n_neighbors=25):\n",
    "    \n",
    "#     neighbors_model = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine', algorithm=\"brute\", n_jobs=-1)\n",
    "#     neighbors_model.fit(V_content.to(\"cuda\"))\n",
    "#     dists, indices = neighbors_model.kneighbors(V_topic)\n",
    "    \n",
    "#     return indices\n",
    "\n",
    "# indices = get_matches(V_answer, V_misconception, n_neighbors=25)\n",
    "\n",
    "indices = cosine_similarity(V_answer.to(\"cpu\"), V_misconception.to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b81971a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T03:11:39.852878Z",
     "iopub.status.busy": "2024-12-01T03:11:39.852117Z",
     "iopub.status.idle": "2024-12-01T03:11:39.863645Z",
     "shell.execute_reply": "2024-12-01T03:11:39.861061Z"
    },
    "papermill": {
     "duration": 0.030377,
     "end_time": "2024-12-01T03:11:39.866897",
     "exception": false,
     "start_time": "2024-12-01T03:11:39.836520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 2587)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee5251c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T03:11:39.895726Z",
     "iopub.status.busy": "2024-12-01T03:11:39.895305Z",
     "iopub.status.idle": "2024-12-01T03:11:39.909168Z",
     "shell.execute_reply": "2024-12-01T03:11:39.908230Z"
    },
    "papermill": {
     "duration": 0.03174,
     "end_time": "2024-12-01T03:11:39.912314",
     "exception": false,
     "start_time": "2024-12-01T03:11:39.880574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2488,  987, 1941, ...,    7, 1187, 2047],\n",
       "       [2488,  987, 1941, ...,    7, 1187, 2047],\n",
       "       [2488,  987, 1005, ...,    7, 1704, 2047],\n",
       "       ...,\n",
       "       [1287,  632,  188, ...,  167, 1399,  690],\n",
       "       [1287,  632,  188, ...,  167, 1399,  690],\n",
       "       [ 632, 1287,  188, ..., 2041, 1574,  690]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sorted_indices  = np.argsort(-indices, axis=1)\n",
    "test_sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "717811c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T03:11:39.943624Z",
     "iopub.status.busy": "2024-12-01T03:11:39.943188Z",
     "iopub.status.idle": "2024-12-01T03:11:40.237827Z",
     "shell.execute_reply": "2024-12-01T03:11:40.236919Z"
    },
    "papermill": {
     "duration": 0.312747,
     "end_time": "2024-12-01T03:11:40.239639",
     "exception": false,
     "start_time": "2024-12-01T03:11:39.926892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# del backbone, model\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f14a5c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T03:11:40.250763Z",
     "iopub.status.busy": "2024-12-01T03:11:40.250497Z",
     "iopub.status.idle": "2024-12-01T03:11:40.296319Z",
     "shell.execute_reply": "2024-12-01T03:11:40.295662Z"
    },
    "papermill": {
     "duration": 0.053106,
     "end_time": "2024-12-01T03:11:40.297929",
     "exception": false,
     "start_time": "2024-12-01T03:11:40.244823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(\"indices.npy\", test_sorted_indices)\n",
    "df.to_parquet(\"df.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc6219",
   "metadata": {
    "papermill": {
     "duration": 0.004638,
     "end_time": "2024-12-01T03:11:40.307409",
     "exception": false,
     "start_time": "2024-12-01T03:11:40.302771",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Picking the best candidate using qwen-32b-instruct-awq\n",
    "Inspired by: https://www.kaggle.com/code/takanashihumbert/eedi-qwen-2-5-32b-awq-two-time-retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8367e",
   "metadata": {
    "papermill": {
     "duration": 0.004678,
     "end_time": "2024-12-01T03:11:40.316765",
     "exception": false,
     "start_time": "2024-12-01T03:11:40.312087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Using MultipleChoiceLogitsProcessor from logits-processor-zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a0413",
   "metadata": {
    "papermill": {
     "duration": 0.004596,
     "end_time": "2024-12-01T03:11:40.326085",
     "exception": false,
     "start_time": "2024-12-01T03:11:40.321489",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/NVIDIA/logits-processor-zoo/refs/heads/main/docs/logo.jpg\" width=\"512\">\n",
    "</p>\n",
    "\n",
    "# logits-processor-zoo\n",
    "\n",
    "Struggling to get LLMs to follow your instructions? LogitsProcessorZoo offers a zoo of tools to use LLMs for specific tasks, beyond just grammar enforcement!\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install logits-processor-zoo\n",
    "```\n",
    "\n",
    "## Supported Frameworks\n",
    "* transformers\n",
    "* vLLM\n",
    "* TensorRT-LLM\n",
    "\n",
    "\n",
    "For the detailed examples in each framework, please have a look at **example_notebook** directory.\n",
    "\n",
    "## Available Logits Processors\n",
    "\n",
    "### GenLengthLogitsProcessor\n",
    "A logits processor that adjusts the likelihood of the end-of-sequence (EOS) token based on the length of the generated sequence, encouraging or discouraging shorter answers.\n",
    "\n",
    "### CiteFromPromptLogitsProcessor\n",
    "A logits processor which boosts or diminishes the likelihood of tokens present in the prompt (and optionally EOS token) to encourage the model to generate tokens similar to those seen in the prompt or vice versa.\n",
    "\n",
    "### ForceLastPhraseLogitsProcessor\n",
    "A logits processor which forces LLMs to use the given phrase before they finalize their answers. Most common use cases can be providing references, thanking user with context etc.\n",
    "\n",
    "### MultipleChoiceLogitsProcessor\n",
    "A logits processor to answer multiple choice questions with one of the choices. A multiple choice question is like:\n",
    "```\n",
    "I am getting a lot of calls during the day. What is more important for me to consider when I buy a new phone?\n",
    "0. Camera\n",
    "1. Screen resolution\n",
    "2. Operating System\n",
    "3. Battery\n",
    "```\n",
    "The goal is to make LLM generate \"3\" as an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ba7638b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T03:11:40.336631Z",
     "iopub.status.busy": "2024-12-01T03:11:40.336375Z",
     "iopub.status.idle": "2024-12-01T03:11:40.341414Z",
     "shell.execute_reply": "2024-12-01T03:11:40.340641Z"
    },
    "papermill": {
     "duration": 0.012193,
     "end_time": "2024-12-01T03:11:40.342970",
     "exception": false,
     "start_time": "2024-12-01T03:11:40.330777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query_text', 'QuestionId_Answer', 'ConstructName', 'SubjectName',\n",
       "       'QuestionText', 'correct_answer', 'incorrect_answer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5bca6d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T03:11:40.354086Z",
     "iopub.status.busy": "2024-12-01T03:11:40.353814Z",
     "iopub.status.idle": "2024-12-01T03:11:40.360882Z",
     "shell.execute_reply": "2024-12-01T03:11:40.359969Z"
    },
    "papermill": {
     "duration": 0.014733,
     "end_time": "2024-12-01T03:11:40.362533",
     "exception": false,
     "start_time": "2024-12-01T03:11:40.347800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_vllm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_vllm.py\n",
    "\n",
    "import vllm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "from typing import List\n",
    "import torch\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "import re\n",
    "\n",
    "model_path       = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "tokenizer        = AutoTokenizer.from_pretrained(model_path)\n",
    "misconception_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "df               = pd.read_parquet(\"df.parquet\")\n",
    "indices          = np.load(\"indices.npy\", allow_pickle=True)\n",
    "PROMPT           = \"\"\"Here is a question about {ConstructName}({SubjectName}).\n",
    "                      Question: {Question}\n",
    "                      Correct Answer: {CorrectAnswer}\n",
    "                      Incorrect Answer: {IncorrectAnswer}\n",
    "                    \n",
    "                      You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\n",
    "                      Answer concisely what misconception it is to lead to getting the incorrect answer.\n",
    "                      Pick the correct misconception number from the below:\n",
    "                    \n",
    "                      {Retrival}\n",
    "                    \"\"\"\n",
    "\n",
    "\n",
    "def preprocess_text(x):\n",
    "    x = re.sub(\"http\\w+\", '',x)   # Delete URL\n",
    "    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    x = re.sub(r\"\\\\\\(\", \" \", x)\n",
    "    x = re.sub(r\"\\\\\\)\", \" \", x)\n",
    "    x = re.sub(r\"[ ]{1,}\", \" \", x)\n",
    "    x = x.strip()                 # Remove empty characters at the beginning and end\n",
    "    return x\n",
    "\n",
    "\n",
    "def apply_template(row, tokenizer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": preprocess_text(\n",
    "                PROMPT.format(\n",
    "                    ConstructName=row[\"ConstructName\"],\n",
    "                    SubjectName=row[\"SubjectName\"],\n",
    "                    Question=row[\"QuestionText\"],\n",
    "                    IncorrectAnswer=row[f\"incorrect_answer\"],\n",
    "                    CorrectAnswer=row[f\"correct_answer\"],\n",
    "                    Retrival=row[f\"retrieval\"]\n",
    "                )\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_candidates(c_indices):\n",
    "    # print(c_indices)\n",
    "    candidates = []\n",
    "    mis_names = misconception_df[\"MisconceptionName\"].values\n",
    "    \n",
    "    for ix in c_indices:\n",
    "        c_names = []\n",
    "        for i, name in enumerate(mis_names[ix]):\n",
    "            c_names.append(f\"{i+1}. {name}\")\n",
    "        candidates.append(\"\\n\".join(c_names))\n",
    "    return candidates\n",
    "\n",
    "\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization           = \"awq\",\n",
    "    tensor_parallel_size   = 2,\n",
    "    gpu_memory_utilization = 0.90, \n",
    "    trust_remote_code      = True,\n",
    "    dtype                  = \"half\", \n",
    "    enforce_eager          = True,\n",
    "    max_model_len          = 5120,\n",
    "    disable_log_stats      = True\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "survivors = indices[:, -1:]\n",
    "\n",
    "for i in range(3):\n",
    "    c_indices       = np.concatenate([indices[:, -8*(i+1)-1:-8*i-1], survivors], axis=1)\n",
    "    df[\"retrieval\"] = get_candidates(c_indices)\n",
    "    df[\"text\"]      = df.apply(lambda row: apply_template(row, tokenizer), axis=1)\n",
    "    \n",
    "    # print(\"Example:\")\n",
    "    # print(df[\"text\"].values[0])\n",
    "    # print()\n",
    "    \n",
    "    responses = llm.generate(\n",
    "        df[\"text\"].values,\n",
    "        vllm.SamplingParams(\n",
    "            n                   = 1,  # Number of output sequences to return for each prompt.\n",
    "            top_k               = 1,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "            temperature         = 0,  # randomness of the sampling\n",
    "            seed                = 777, # Seed for reprodicibility\n",
    "            skip_special_tokens = False,  # Whether to skip special tokens in the output.\n",
    "            max_tokens          = 1,  # Maximum number of tokens to generate per output sequence.\n",
    "            logits_processors   = [MultipleChoiceLogitsProcessor(tokenizer, choices=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])]\n",
    "                            ),\n",
    "        use_tqdm=True\n",
    "    )\n",
    "    \n",
    "    responses      = [x.outputs[0].text for x in responses]\n",
    "    df[\"response\"] = responses\n",
    "    llm_choices    = df[\"response\"].astype(int).values - 1\n",
    "    survivors      = np.array([cix[best] for best, cix in zip(llm_choices, c_indices)]).reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "for i in range(indices.shape[0]):\n",
    "    ix         = indices[i]\n",
    "    llm_choice = survivors[i, 0]\n",
    "    results.append(\" \".join([str(llm_choice)] + [str(x) for x in ix if x != llm_choice]))\n",
    "\n",
    "\n",
    "df[\"MisconceptionId\"] = results\n",
    "df.to_csv(\"submission.csv\", columns=[\"QuestionId_Answer\", \"MisconceptionId\"], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4dbeb3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T03:11:40.373234Z",
     "iopub.status.busy": "2024-12-01T03:11:40.372979Z",
     "iopub.status.idle": "2024-12-01T03:14:07.753688Z",
     "shell.execute_reply": "2024-12-01T03:14:07.752720Z"
    },
    "papermill": {
     "duration": 147.388385,
     "end_time": "2024-12-01T03:14:07.755815",
     "exception": false,
     "start_time": "2024-12-01T03:11:40.367430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "WARNING 12-01 03:11:44 config.py:246] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 12-01 03:11:44 config.py:715] Defaulting to use mp for distributed inference\r\n",
      "INFO 12-01 03:11:44 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1, use_v2_block_manager=False, enable_prefix_caching=False)\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "INFO 12-01 03:11:45 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "INFO 12-01 03:11:45 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=127)\u001b[0;0m INFO 12-01 03:11:45 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=127)\u001b[0;0m INFO 12-01 03:11:45 selector.py:54] Using XFormers backend.\r\n",
      "INFO 12-01 03:11:45 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=127)\u001b[0;0m INFO 12-01 03:11:46 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=127)\u001b[0;0m INFO 12-01 03:11:46 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "INFO 12-01 03:11:46 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=127)\u001b[0;0m INFO 12-01 03:11:46 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-01 03:11:46 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-01 03:11:47 custom_all_reduce_utils.py:202] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-01 03:11:54 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=127)\u001b[0;0m INFO 12-01 03:11:54 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "WARNING 12-01 03:11:54 custom_all_reduce.py:127] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=127)\u001b[0;0m WARNING 12-01 03:11:54 custom_all_reduce.py:127] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "INFO 12-01 03:11:54 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7ba380e1e320>, local_subscribe_port=51071, local_sync_port=57869, remote_subscribe_port=None, remote_sync_port=None)\r\n",
      "INFO 12-01 03:11:54 model_runner.py:680] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=127)\u001b[0;0m INFO 12-01 03:11:54 model_runner.py:680] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=127)\u001b[0;0m INFO 12-01 03:11:54 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=127)\u001b[0;0m INFO 12-01 03:11:54 selector.py:54] Using XFormers backend.\r\n",
      "INFO 12-01 03:11:54 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-01 03:11:54 selector.py:54] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:18<01:14, 18.68s/it]\r\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:37<00:56, 18.95s/it]\r\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:58<00:39, 19.59s/it]\r\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [01:19<00:20, 20.33s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [01:40<00:00, 20.52s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [01:40<00:00, 20.10s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=127)\u001b[0;0m INFO 12-01 03:13:35 model_runner.py:692] Loading model weights took 9.0933 GB\r\n",
      "INFO 12-01 03:13:35 model_runner.py:692] Loading model weights took 9.0933 GB\r\n",
      "INFO 12-01 03:13:43 distributed_gpu_executor.py:56] # GPU blocks: 815, # CPU blocks: 2048\r\n",
      "Processed prompts: 100%|█| 9/9 [00:05<00:00,  1.56it/s, est. speed input: 482.24\r\n",
      "Processed prompts: 100%|█| 9/9 [00:05<00:00,  1.67it/s, est. speed input: 527.20\r\n",
      "Processed prompts: 100%|█| 9/9 [00:05<00:00,  1.62it/s, est. speed input: 529.00\r\n"
     ]
    }
   ],
   "source": [
    "!python run_vllm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11ab1ca5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T03:14:07.770155Z",
     "iopub.status.busy": "2024-12-01T03:14:07.769863Z",
     "iopub.status.idle": "2024-12-01T03:14:07.783838Z",
     "shell.execute_reply": "2024-12-01T03:14:07.783106Z"
    },
    "papermill": {
     "duration": 0.022857,
     "end_time": "2024-12-01T03:14:07.785338",
     "exception": false,
     "start_time": "2024-12-01T03:14:07.762481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>1210 2488 987 1941 1672 2518 1005 2532 706 150...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>1210 2488 987 1941 1672 2518 1005 2532 706 150...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>7 2488 987 1005 1672 1941 706 1507 2518 2532 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>242 885 1593 979 2398 1540 80 2307 113 1256 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>2510 885 1593 1540 2398 979 80 113 2307 1256 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>988 885 1593 1540 2398 979 80 2307 113 2078 59...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>2069 1287 632 188 397 1677 2211 2471 1349 2319...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>2069 1287 632 188 397 1677 2211 2471 1349 1923...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>926 632 1287 188 1677 397 1059 1073 2471 2456 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  1210 2488 987 1941 1672 2518 1005 2532 706 150...\n",
       "1            1869_C  1210 2488 987 1941 1672 2518 1005 2532 706 150...\n",
       "2            1869_D  7 2488 987 1005 1672 1941 706 1507 2518 2532 3...\n",
       "3            1870_A  242 885 1593 979 2398 1540 80 2307 113 1256 20...\n",
       "4            1870_B  2510 885 1593 1540 2398 979 80 113 2307 1256 2...\n",
       "5            1870_C  988 885 1593 1540 2398 979 80 2307 113 2078 59...\n",
       "6            1871_A  2069 1287 632 188 397 1677 2211 2471 1349 2319...\n",
       "7            1871_C  2069 1287 632 188 397 1677 2211 2471 1349 1923...\n",
       "8            1871_D  926 632 1287 188 1677 397 1059 1073 2471 2456 ..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b56577",
   "metadata": {
    "papermill": {
     "duration": 0.005944,
     "end_time": "2024-12-01T03:14:07.797489",
     "exception": false,
     "start_time": "2024-12-01T03:14:07.791545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9082668f",
   "metadata": {
    "papermill": {
     "duration": 0.005903,
     "end_time": "2024-12-01T03:14:07.809563",
     "exception": false,
     "start_time": "2024-12-01T03:14:07.803660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5251603,
     "sourceId": 9094368,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5922197,
     "sourceId": 9687536,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5920031,
     "sourceId": 9688062,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5957531,
     "sourceId": 9734430,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6117312,
     "sourceId": 9948011,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4581967,
     "sourceId": 9952014,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 200567623,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 123481,
     "modelInstanceId": 99392,
     "sourceId": 118192,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 454.774707,
   "end_time": "2024-12-01T03:14:10.748608",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-01T03:06:35.973901",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "134a97ccb6744d9b8ca37a8870b61354": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1732a3165d8f418aa30632f0311d6ec9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7cd65442cb3743ed85de2ce17e7fb2c1",
       "placeholder": "​",
       "style": "IPY_MODEL_1cd7e5265e464c3995261eb3ae310f34",
       "value": "Batches: 100%"
      }
     },
     "1949a14fb130424a8749026b2b425a28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2cab64b7ab4649ef9fce2b60f83fc4e9",
       "placeholder": "​",
       "style": "IPY_MODEL_6ebd220574564f6e95547ddc4d8b2606",
       "value": "Batches: 100%"
      }
     },
     "1cd7e5265e464c3995261eb3ae310f34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2cab64b7ab4649ef9fce2b60f83fc4e9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3d0f0e3e43ec494097bab07efd5d8b0b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4f8d9fe9892842c1a144690982ce4fd0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5d709676ebeb4445a982697a94f02214": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "68bf3259e8824fe7877d8bd89833ad8c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6c27289a35d9494db55037bf53d0cbec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_134a97ccb6744d9b8ca37a8870b61354",
       "placeholder": "​",
       "style": "IPY_MODEL_baa08cffbaeb4208a9ce07d9ae452be6",
       "value": " 81/81 [00:09&lt;00:00, 11.68it/s]"
      }
     },
     "6ebd220574564f6e95547ddc4d8b2606": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "760e6f19fb2844348cf4b362da45c8e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7cd65442cb3743ed85de2ce17e7fb2c1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "88e5017c36d94732ae22fda75ae45afc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e501e69c7cd34bf79d68142b4a12ca28",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_cb0e4a00c5ef4e1c96b7333dd2e1c4a4",
       "value": 1.0
      }
     },
     "a39e4998e0d5464ca80fef28281bc1a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "baa08cffbaeb4208a9ce07d9ae452be6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c13d8fc600674fcf997bbf3731b99996": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1732a3165d8f418aa30632f0311d6ec9",
        "IPY_MODEL_e8aa3c19680b41fc83b3329d587b1bb3",
        "IPY_MODEL_6c27289a35d9494db55037bf53d0cbec"
       ],
       "layout": "IPY_MODEL_5d709676ebeb4445a982697a94f02214"
      }
     },
     "cb0e4a00c5ef4e1c96b7333dd2e1c4a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d55a6a25d554499bbcdec0b7c81825fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1949a14fb130424a8749026b2b425a28",
        "IPY_MODEL_88e5017c36d94732ae22fda75ae45afc",
        "IPY_MODEL_f5e7e35d69c0468dac510b21b391ae1f"
       ],
       "layout": "IPY_MODEL_3d0f0e3e43ec494097bab07efd5d8b0b"
      }
     },
     "e501e69c7cd34bf79d68142b4a12ca28": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e8aa3c19680b41fc83b3329d587b1bb3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4f8d9fe9892842c1a144690982ce4fd0",
       "max": 81.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_760e6f19fb2844348cf4b362da45c8e1",
       "value": 81.0
      }
     },
     "f5e7e35d69c0468dac510b21b391ae1f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_68bf3259e8824fe7877d8bd89833ad8c",
       "placeholder": "​",
       "style": "IPY_MODEL_a39e4998e0d5464ca80fef28281bc1a2",
       "value": " 1/1 [00:01&lt;00:00,  1.49s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
